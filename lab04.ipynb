{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab04",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPprk2gCovIQHVI8JSlv2mV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDDMIN/TensorFlow/blob/main/lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTjCvLhjZrrT",
        "outputId": "411ac35c-95e8-4135-a6f4-d958737522cb"
      },
      "source": [
        "#multi variable linear regression\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "#data and label\r\n",
        "x1 = [73., 93., 89., 96., 73.]\r\n",
        "x2 = [80., 88., 91., 98., 66.]\r\n",
        "x3 = [75., 93., 90., 100., 70.]\r\n",
        "Y = [152., 185., 180., 196., 142.]\r\n",
        "\r\n",
        "#weights\r\n",
        "w1 = tf.Variable(tf.random.normal([1]))\r\n",
        "w2 = tf.Variable(tf.random.normal([1]))\r\n",
        "w3 = tf.Variable(tf.random.normal([1]))\r\n",
        "b = tf.Variable(tf.random.normal([1]))\r\n",
        "\r\n",
        "learning_rate = 0.000001\r\n",
        "\r\n",
        "for i in range(1000 + 1):\r\n",
        "  #tf.GradientTape() to record the gradient of the cost function\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\r\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\r\n",
        "  #calculates the gradients of the cost\r\n",
        "  w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\r\n",
        "\r\n",
        "  #upgrade w1, w2, w3, and b  #미분한 값*러닝레이트를 빼면 새로운 W나옴\r\n",
        "  w1.assign_sub(learning_rate * w1_grad) #새로운 W를 할당\r\n",
        "  w2.assign_sub(learning_rate * w2_grad)\r\n",
        "  w3.assign_sub(learning_rate * w3_grad)\r\n",
        "  b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "  if i % 50 == 0:\r\n",
        "    print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |    9832.0381\n",
            "   50 |     127.3318\n",
            "  100 |      19.5987\n",
            "  150 |      18.3546\n",
            "  200 |      18.2921\n",
            "  250 |      18.2429\n",
            "  300 |      18.1940\n",
            "  350 |      18.1452\n",
            "  400 |      18.0967\n",
            "  450 |      18.0482\n",
            "  500 |      17.9998\n",
            "  550 |      17.9515\n",
            "  600 |      17.9034\n",
            "  650 |      17.8554\n",
            "  700 |      17.8075\n",
            "  750 |      17.7598\n",
            "  800 |      17.7122\n",
            "  850 |      17.6647\n",
            "  900 |      17.6173\n",
            "  950 |      17.5702\n",
            " 1000 |      17.5231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOZ7KsbFdlJV",
        "outputId": "509e768a-54e1-47ed-c430-a7d489cf26e1"
      },
      "source": [
        "#multi variable linear regression/Matrix 사용\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "data = np.array([\r\n",
        "    # X1, X2, X3, y\r\n",
        "    [73., 80., 75., 152.],\r\n",
        "    [93., 88., 93., 185.],\r\n",
        "    [89., 91., 90., 180.],\r\n",
        "    [96., 98., 100., 196.],\r\n",
        "    [73., 66., 70., 142.]\r\n",
        "], dtype = np.float32)\r\n",
        "\r\n",
        "#slice data[행, 열] :기준 앞 뒤까지\r\n",
        "X = data[:, :-1] #모든 행의 마지막 열 제외 (x1, x2, x3)\r\n",
        "y = data[:, [-1]] #모든 행의 마지막 열(y)\r\n",
        "\r\n",
        "\r\n",
        "W = tf.Variable(tf.random.normal([3, 1])) #3행 1열의 weight 생성 \r\n",
        "b = tf.Variable(tf.random.normal([1])) #biased 상수 생성\r\n",
        "\r\n",
        "learning_rate = 0.000001\r\n",
        "\r\n",
        "#hyphothesis, prediction function\r\n",
        "def predict(X):\r\n",
        "  return tf.matmul(X, W) + b\r\n",
        "\r\n",
        "n_epochs = 2000\r\n",
        "for i in range(n_epochs+1):\r\n",
        "  #record the gradient of the cost function\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    cost = tf.reduce_mean((tf.square(predict(X) - y)))\r\n",
        "\r\n",
        "    #calculate the gradients of the Loss\r\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\r\n",
        "\r\n",
        "    #updates parameters (W and b)\r\n",
        "    W.assign_sub(learning_rate * W_grad)\r\n",
        "    b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "    if i % 100 == 0:\r\n",
        "      print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |    21.5019\n",
            "  100 |     3.5439\n",
            "  200 |     3.5387\n",
            "  300 |     3.5357\n",
            "  400 |     3.5328\n",
            "  500 |     3.5298\n",
            "  600 |     3.5269\n",
            "  700 |     3.5239\n",
            "  800 |     3.5210\n",
            "  900 |     3.5180\n",
            " 1000 |     3.5151\n",
            " 1100 |     3.5122\n",
            " 1200 |     3.5093\n",
            " 1300 |     3.5064\n",
            " 1400 |     3.5035\n",
            " 1500 |     3.5006\n",
            " 1600 |     3.4978\n",
            " 1700 |     3.4949\n",
            " 1800 |     3.4921\n",
            " 1900 |     3.4892\n",
            " 2000 |     3.4864\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}